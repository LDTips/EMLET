{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "The aim is to implement a ML solution that will perform better than the provided example jupyter notebook that utilises basic logistic regression from the scikit module. The data was scaled and then fit into that model. The following results were obtained:\n",
    "# LogisticRegression\n",
    "## LogisticRegression (34 classes)\n",
    "accuracy_score:  0.802\\\n",
    "recall_score:  0.595\\\n",
    "precision_score:  0.487\\\n",
    "f1_score:  0.494\n",
    "\n",
    "## LogisticRegression (8 classes)\n",
    "accuracy_score =  0.832\\\n",
    "recall_score =  0.696\\\n",
    "precision_score =  0.512\\\n",
    "f1_score =  0.539\n",
    "\n",
    "## LogisticRegression (2 classes)\n",
    "accuracy_score:  0.989\\\n",
    "recall_score:  0.890\\\n",
    "precision_score:  0.864\\\n",
    "f1_score:  0.877\n",
    "\n",
    "# SGD\n",
    "## SGD (34 classes)\n",
    "accuracy_score:  0.7860381477619159\n",
    "recall_score:  0.5179118961920882\n",
    "precision_score:  0.42184457529453795\n",
    "f1_score:  0.4256465870416849\n",
    "\n",
    "## SGD (8 classes)\n",
    "accuracy_score =  0.8226369976250854\n",
    "recall_score =  0.68058577451911\n",
    "precision_score =  0.4617525150801485\n",
    "f1_score =  0.49719100438594754\n",
    "\n",
    "## SGD (2 classes)\n",
    "accuracy_score:  0.9868773803425305\n",
    "recall_score:  0.8686111308207715\n",
    "precision_score:  0.8334084807767805\n",
    "f1_score:  0.8500759333973384\n",
    "\n",
    "\n",
    "It is worth noting that good accuracy score in this case is not a good metric due to high amount of malicious packet data compared to benign. To combat that, it is better to look at score metrics such as recall, precision and f1. Furthermore, duplication of benign entries is an option, but might not be a correct option due to the actual nature of DDoS attacks having very big amount of packets compared to normal benign traffic."
   ],
   "id": "a5b022abca638b7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Coding\n",
    "## Importing packages and dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3efd98ab4f82110b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T09:56:34.906678400Z",
     "start_time": "2024-06-01T09:56:34.884676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "DATASET_DIR = r'../CICIoT2023'"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T09:56:34.965677Z",
     "start_time": "2024-06-01T09:56:34.923676200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_sets = [k for k in os.listdir(DATASET_DIR) if k.endswith('.csv')]\n",
    "df_sets.sort()\n",
    "# Data is too large to parse as a single pd\n",
    "index_range = range(len(df_sets))\n",
    "import random\n",
    "SEED = 42  # Set to None for randomness\n",
    "if SEED:\n",
    "    random.seed(SEED)\n",
    "    print(f\"INFO: Using seed {SEED}\")\n",
    "else:\n",
    "    print(f\"Using random seed\")\n",
    "CLASSES = 7 ## Valid values are 34, 7 or 2\n",
    "if CLASSES not in (34, 7, 2):\n",
    "    print(\"Please set a valid number of classes (34, 7, 2)\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"INFO: Using {CLASSES} classes\")\n",
    "# Manual train/test splitting\n",
    "test_indexes_count = int(len(df_sets)*0.2)\n",
    "train_indexes_count = int(len(df_sets)*0.8)\n",
    "test_indexes = random.sample(index_range, test_indexes_count)\n",
    "train_indexes = [i for i in index_range if i not in test_indexes]\n",
    "test_data, train_data = [], []\n",
    "print(test_indexes)\n",
    "X_columns = [\n",
    "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
    "    'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
    "    'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
    "    'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "    'syn_count', 'fin_count', 'urg_count', 'rst_count', \n",
    "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
    "    'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
    "    'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
    "    'Radius', 'Covariance', 'Variance', 'Weight', \n",
    "]\n",
    "y_column = 'label'"
   ],
   "id": "6aa1186881b995ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using seed 42\n",
      "INFO: Using 7 classes\n",
      "[163, 28, 6, 70, 62, 57, 35, 26, 139, 22, 151, 108, 8, 7, 23, 55, 59, 129, 166, 143, 50, 160, 107, 56, 114, 71, 1, 40, 157, 87, 149, 39, 153]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scaling the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d152f068c80c6c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T09:56:34.965677Z",
     "start_time": "2024-06-01T09:56:34.941678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "scaler_filename = f\"precomputed/scaler-{SEED}.save\"\n",
    "\n",
    "if os.path.exists(scaler_filename):\n",
    "    scaler = joblib.load(scaler_filename) \n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    for train_set in tqdm(train_indexes):\n",
    "        scaler.fit(pd.read_csv(DATASET_DIR + '/' + df_sets[train_set])[X_columns])\n",
    "\n",
    "    joblib.dump(scaler, scaler_filename)"
   ],
   "id": "1f9eaece35701381",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialise pytorch and check if GPU is available"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42265f56f2e93eb9"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected calculation device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "if SEED:\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Detected calculation device: {DEVICE}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T09:56:34.976676100Z",
     "start_time": "2024-06-01T09:56:34.958677800Z"
    }
   },
   "id": "7139f3bac7c8fb0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the custom neural network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "990db8b0a4469774"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(hidden_size//2),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(hidden_size//4),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_size//2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Determine the model sizes\n",
    "dim_check_dataset = pd.read_csv(DATASET_DIR + '/' + df_sets[0])\n",
    "input_size = dim_check_dataset[X_columns].shape[1]  # ANN input size is count of variables\n",
    "hidden_size = 64\n",
    "output_size = len(dim_check_dataset[y_column].unique())  # Output size is number of possible labels\n",
    "del dim_check_dataset\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = CustomClassifier(input_size, hidden_size, output_size).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "label_encoder = LabelEncoder()\n",
    "pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T09:56:35.929675900Z",
     "start_time": "2024-06-01T09:56:34.973677Z"
    }
   },
   "id": "47bc814b13e11ca2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main training loop\n",
    "The data is read every iteration due to the weight of the whole data not being able to fit to memory at the same time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d5dac3a205e45e5"
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-01T09:56:35.944677400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define class count overrides\n",
    "if True:\n",
    "    dict_7classes = {}\n",
    "    dict_7classes['DDoS-RSTFINFlood'] = 'DDoS'\n",
    "    dict_7classes['DDoS-PSHACK_Flood'] = 'DDoS'\n",
    "    dict_7classes['DDoS-SYN_Flood'] = 'DDoS'\n",
    "    dict_7classes['DDoS-UDP_Flood'] = 'DDoS'\n",
    "    dict_7classes['DDoS-TCP_Flood'] = 'DDoS'\n",
    "    dict_7classes['DDoS-ICMP_Flood'] = 'DDoS'\n",
    "    dict_7classes['DDoS-SynonymousIP_Flood'] = 'DDoS'\n",
    "    dict_7classes['DDoS-ACK_Fragmentation'] = 'DDoS'\n",
    "    dict_7classes['DDoS-UDP_Fragmentation'] = 'DDoS'\n",
    "    dict_7classes['DDoS-ICMP_Fragmentation'] = 'DDoS'\n",
    "    dict_7classes['DDoS-SlowLoris'] = 'DDoS'\n",
    "    dict_7classes['DDoS-HTTP_Flood'] = 'DDoS'\n",
    "    \n",
    "    dict_7classes['DoS-UDP_Flood'] = 'DoS'\n",
    "    dict_7classes['DoS-SYN_Flood'] = 'DoS'\n",
    "    dict_7classes['DoS-TCP_Flood'] = 'DoS'\n",
    "    dict_7classes['DoS-HTTP_Flood'] = 'DoS'\n",
    "    \n",
    "    \n",
    "    dict_7classes['Mirai-greeth_flood'] = 'Mirai'\n",
    "    dict_7classes['Mirai-greip_flood'] = 'Mirai'\n",
    "    dict_7classes['Mirai-udpplain'] = 'Mirai'\n",
    "    \n",
    "    dict_7classes['Recon-PingSweep'] = 'Recon'\n",
    "    dict_7classes['Recon-OSScan'] = 'Recon'\n",
    "    dict_7classes['Recon-PortScan'] = 'Recon'\n",
    "    dict_7classes['VulnerabilityScan'] = 'Recon'\n",
    "    dict_7classes['Recon-HostDiscovery'] = 'Recon'\n",
    "    \n",
    "    dict_7classes['DNS_Spoofing'] = 'Spoofing'\n",
    "    dict_7classes['MITM-ArpSpoofing'] = 'Spoofing'\n",
    "    \n",
    "    dict_7classes['BenignTraffic'] = 'Benign'\n",
    "    \n",
    "    dict_7classes['BrowserHijacking'] = 'Web'\n",
    "    dict_7classes['Backdoor_Malware'] = 'Web'\n",
    "    dict_7classes['XSS'] = 'Web'\n",
    "    dict_7classes['Uploading_Attack'] = 'Web'\n",
    "    dict_7classes['SqlInjection'] = 'Web'\n",
    "    dict_7classes['CommandInjection'] = 'Web'\n",
    "    \n",
    "    \n",
    "    dict_7classes['DictionaryBruteForce'] = 'BruteForce'\n",
    "    dict_2classes = {}\n",
    "    dict_2classes['DDoS-RSTFINFlood'] = 'Attack'\n",
    "    dict_2classes['DDoS-PSHACK_Flood'] = 'Attack'\n",
    "    dict_2classes['DDoS-SYN_Flood'] = 'Attack'\n",
    "    dict_2classes['DDoS-UDP_Flood'] = 'Attack'\n",
    "    dict_2classes['DDoS-TCP_Flood'] = 'Attack'\n",
    "    dict_2classes['DDoS-ICMP_Flood'] = 'Attack'\n",
    "    dict_2classes['DDoS-SynonymousIP_Flood'] = 'Attack'\n",
    "    dict_2classes['DDoS-ACK_Fragmentation'] = 'Attack'\n",
    "    dict_2classes['DDoS-UDP_Fragmentation'] = 'Attack'\n",
    "    dict_2classes['DDoS-ICMP_Fragmentation'] = 'Attack'\n",
    "    dict_2classes['DDoS-SlowLoris'] = 'Attack'\n",
    "    dict_2classes['DDoS-HTTP_Flood'] = 'Attack'\n",
    "    \n",
    "    dict_2classes['DoS-UDP_Flood'] = 'Attack'\n",
    "    dict_2classes['DoS-SYN_Flood'] = 'Attack'\n",
    "    dict_2classes['DoS-TCP_Flood'] = 'Attack'\n",
    "    dict_2classes['DoS-HTTP_Flood'] = 'Attack'\n",
    "    \n",
    "    \n",
    "    dict_2classes['Mirai-greeth_flood'] = 'Attack'\n",
    "    dict_2classes['Mirai-greip_flood'] = 'Attack'\n",
    "    dict_2classes['Mirai-udpplain'] = 'Attack'\n",
    "    \n",
    "    dict_2classes['Recon-PingSweep'] = 'Attack'\n",
    "    dict_2classes['Recon-OSScan'] = 'Attack'\n",
    "    dict_2classes['Recon-PortScan'] = 'Attack'\n",
    "    dict_2classes['VulnerabilityScan'] = 'Attack'\n",
    "    dict_2classes['Recon-HostDiscovery'] = 'Attack'\n",
    "    \n",
    "    dict_2classes['DNS_Spoofing'] = 'Attack'\n",
    "    dict_2classes['MITM-ArpSpoofing'] = 'Attack'\n",
    "    \n",
    "    dict_2classes['BenignTraffic'] = 'Benign'\n",
    "    \n",
    "    dict_2classes['BrowserHijacking'] = 'Attack'\n",
    "    dict_2classes['Backdoor_Malware'] = 'Attack'\n",
    "    dict_2classes['XSS'] = 'Attack'\n",
    "    dict_2classes['Uploading_Attack'] = 'Attack'\n",
    "    dict_2classes['SqlInjection'] = 'Attack'\n",
    "    dict_2classes['CommandInjection'] = 'Attack'\n",
    "    \n",
    "    dict_2classes['DictionaryBruteForce'] = 'Attack'  # Defin # Def\n",
    "\n",
    "num_epochs = len(df_sets)\n",
    "prev_loss = float('inf')\n",
    "\n",
    "for epoch, t_i in enumerate(tqdm(train_indexes[:len(train_indexes)])):\n",
    "    iter_data = pd.read_csv(DATASET_DIR + '/' + df_sets[t_i])\n",
    "    optimizer.zero_grad()  # Reset the gradients before iteration\n",
    "    \n",
    "    # Generate synthetic data for this iteration\n",
    "    iter_data[X_columns] = scaler.transform(iter_data[X_columns])\n",
    "    X = iter_data[X_columns].values  # Convert DataFrame to numpy array\n",
    "    if CLASSES == 7:\n",
    "        y = np.array([dict_7classes[k] for k in iter_data[y_column]])\n",
    "    elif CLASSES == 2:\n",
    "        y = np.array([dict_2classes[k] for k in iter_data[y_column]])\n",
    "    else:\n",
    "        y = iter_data[y_column].values\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)  # Convert numpy array to torch tensor\n",
    "    \n",
    "    y_encoded = label_encoder.fit_transform(y)  # Encode labels (convert them from string)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.long).to(DEVICE)\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if prev_loss - loss.item() < 0.01:\n",
    "        tqdm.write(f'Ending training on epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "        break\n",
    "    if (epoch+1) % (train_indexes_count//10) == 0:\n",
    "        tqdm.write(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n"
   ],
   "id": "8333e82f5deb1c38",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 6/136 [01:14<32:33, 15.02s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfeaffa2aed3713"
  },
  {
   "metadata": {
    "is_executing": true
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "pass\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "val_losses = []\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_set in tqdm(test_indexes):\n",
    "        iter_data = pd.read_csv(DATASET_DIR + '/' + df_sets[test_set])\n",
    "        iter_data[X_columns] = scaler.transform(iter_data[X_columns])\n",
    "        X = iter_data[X_columns].values  # Convert DataFrame to numpy array\n",
    "        if CLASSES == 7:\n",
    "            y = np.array([dict_7classes[k] for k in iter_data[y_column]])\n",
    "        elif CLASSES == 2:\n",
    "            y = np.array([dict_2classes[k] for k in iter_data[y_column]])\n",
    "        else:\n",
    "            y = iter_data[y_column].values\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)  # Convert numpy array to tensor\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        y_tensor = torch.tensor(y_encoded, dtype=torch.long).to(DEVICE)\n",
    "        \n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        # Convert logits to class predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        true_labels.extend(y_tensor.tolist())\n",
    "        pred_labels.extend(predicted.tolist())\n",
    "    \n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='macro')\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(f'Validation Loss: {np.mean(val_losses):.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "from uuid import uuid4\n",
    "import inspect\n",
    "current_uuid = str(uuid4())\n",
    "print(f\"Saving model with uuid {current_uuid}\")\n",
    "# TODO - improve the writeout (include seed if set, include parameters e.g. hidden_layer sizes etc etc)\n",
    "with open(f'results/{current_uuid}', 'w') as f:\n",
    "    if SEED:\n",
    "        f.write(f'SEED was set: {SEED}\\n')\n",
    "    f.write(f'Input, Hidden and Output: {input_size}, {hidden_size}, {output_size}')\n",
    "    f.write(f'\\n\\nValidation Loss: {np.mean(val_losses):.4f}\\nAccuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1 Score: {f1:.4f}\\n\\n')\n",
    "    # Hacky way of saving the model class for reuse\n",
    "    v = vars(model.__class__)\n",
    "    methods = [name for name, attr in v.items() if inspect.isfunction(attr)]\n",
    "    f.write('class CustomClassifier(nn.Module):\\n')\n",
    "    for m in methods:\n",
    "        f.write(inspect.getsource(getattr(model, m)))"
   ],
   "id": "73e44369e4cf6bb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix_seaborn.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Convert confusion matrix array to a DataFrame\n",
    "cm_df = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "cm_df.to_csv('confusion_matrix.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "778d713860421a77"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
